{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-20T16:00:17.582153Z",
     "iopub.status.busy": "2024-11-20T16:00:17.581397Z",
     "iopub.status.idle": "2024-11-20T16:00:29.497788Z",
     "shell.execute_reply": "2024-11-20T16:00:29.496485Z",
     "shell.execute_reply.started": "2024-11-20T16:00:17.582119Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install evaluate rouge-score sentence-transformers > /dev/null 2>&1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T16:00:29.500257Z",
     "iopub.status.busy": "2024-11-20T16:00:29.499961Z",
     "iopub.status.idle": "2024-11-20T16:00:42.761938Z",
     "shell.execute_reply": "2024-11-20T16:00:42.761046Z",
     "shell.execute_reply.started": "2024-11-20T16:00:29.500223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import evaluate\n",
    "from sklearn.cluster import DBSCAN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import random\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T16:00:42.763741Z",
     "iopub.status.busy": "2024-11-20T16:00:42.763037Z",
     "iopub.status.idle": "2024-11-20T16:00:43.854398Z",
     "shell.execute_reply": "2024-11-20T16:00:43.853488Z",
     "shell.execute_reply.started": "2024-11-20T16:00:42.763699Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2911f40ee041cd82f2c9a9dd2d2663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge=evaluate.load(\"rouge\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T16:00:43.857411Z",
     "iopub.status.busy": "2024-11-20T16:00:43.857050Z",
     "iopub.status.idle": "2024-11-20T16:00:43.863226Z",
     "shell.execute_reply": "2024-11-20T16:00:43.862360Z",
     "shell.execute_reply.started": "2024-11-20T16:00:43.857373Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RelRegModel(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super(RelRegModel, self).__init__()\n",
    "        self.encoder = BertModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.encoder(input_ids=input_ids, \n",
    "                               attention_mask=attention_mask, \n",
    "                               token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        score = self.regressor(pooled_output)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T16:00:43.864691Z",
     "iopub.status.busy": "2024-11-20T16:00:43.864316Z",
     "iopub.status.idle": "2024-11-20T16:00:49.357031Z",
     "shell.execute_reply": "2024-11-20T16:00:49.356207Z",
     "shell.execute_reply.started": "2024-11-20T16:00:43.864636Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/3652340851.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_relreg=torch.load('../input/anlp-relreg/RelReg.pth')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6268d433b9cd44cf9d467ca0a4bb4f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700787f27af1465791295687fb0110d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70181799340048b3b65e6cf9466da02b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036f26d6e4b64eb4bddd017be6d0a1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_relreg=torch.load('../input/anlp-relreg/RelReg.pth')\n",
    "tokenizer_relreg = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T16:00:49.358286Z",
     "iopub.status.busy": "2024-11-20T16:00:49.358031Z",
     "iopub.status.idle": "2024-11-20T16:00:55.236432Z",
     "shell.execute_reply": "2024-11-20T16:00:55.235700Z",
     "shell.execute_reply.started": "2024-11-20T16:00:49.358262Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7491459b5d142309dc68a848f87397e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1125c4bf9a464584b86c54ec8eb29fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd08cf5f59145ca833109c7cd83472c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33420d2a79074027acb61646423b9493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5814027f7c484e3fba0147c67c7fc970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc30f6e401e4361b5c0211651d6ad68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bc2f29545a4f26bd669edb114f4bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e7375ba4aed46de95934c3ea253b283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb68c62e2c9e4c49a08d104487170854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c36f7f5a0f4552a6dce7d188c4524a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf2e164663394b16980d45a2e767d46d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T16:00:55.237626Z",
     "iopub.status.busy": "2024-11-20T16:00:55.237395Z",
     "iopub.status.idle": "2024-11-20T16:00:59.288532Z",
     "shell.execute_reply": "2024-11-20T16:00:59.287629Z",
     "shell.execute_reply.started": "2024-11-20T16:00:55.237603Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d14dade15448cb83f3f1a18b5cf8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/9.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0113ab3bfae440518ec2d5687bd75171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl:   0%|          | 0.00/24.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7335475401684c87aadafdd843dd7195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.jsonl:   0%|          | 0.00/4.43M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fdec22d47948bdb1b35ed6a5c7f96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl:   0%|          | 0.00/8.76M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7b6e6ad18d4d0cba7c54bc91173b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2783 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0059bf9123cf40339654d7661dba06c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249c77d93dcb47aa8ec8413f5efaebd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answersumm = load_dataset(\"alexfabbri/answersumm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T16:00:59.290581Z",
     "iopub.status.busy": "2024-11-20T16:00:59.289980Z",
     "iopub.status.idle": "2024-11-20T16:00:59.294956Z",
     "shell.execute_reply": "2024-11-20T16:00:59.294026Z",
     "shell.execute_reply.started": "2024-11-20T16:00:59.290540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_rouge_score(sent,ref_summary):\n",
    "    results=rouge.compute(predictions=[sent], references=[ref_summary])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T16:00:59.296539Z",
     "iopub.status.busy": "2024-11-20T16:00:59.296210Z",
     "iopub.status.idle": "2024-11-20T16:01:14.007369Z",
     "shell.execute_reply": "2024-11-20T16:01:14.006594Z",
     "shell.execute_reply.started": "2024-11-20T16:00:59.296503Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b8b49afc4949e7a4e9e0eeb0684c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7978a90c190c4f0a85e0b6feb881a3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1530a05adce44c4182f30cc95fa03b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86ccc62946d341dbb481641c0db74af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd2a12bf5774b15809d4f1f4d5e1809",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/1976436074.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model2=torch.load('../input/bart-ft2/BART_FT2.pth')\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer2 = BartTokenizer.from_pretrained(model_name)\n",
    "model2=torch.load('../input/bart-ft2/BART_FT2.pth')\n",
    "model2=model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T16:01:14.010226Z",
     "iopub.status.busy": "2024-11-20T16:01:14.009969Z",
     "iopub.status.idle": "2024-11-20T16:01:14.015227Z",
     "shell.execute_reply": "2024-11-20T16:01:14.014180Z",
     "shell.execute_reply.started": "2024-11-20T16:01:14.010202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_summary(input_text):\n",
    "    inputs = tokenizer2(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model2.generate(inputs[\"input_ids\"].to(device), max_length=256, min_length=10, length_penalty=2.0, num_beams=4)\n",
    "    summary = tokenizer2.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-20T16:02:53.617102Z",
     "iopub.status.busy": "2024-11-20T16:02:53.616342Z",
     "iopub.status.idle": "2024-11-20T16:15:24.446131Z",
     "shell.execute_reply": "2024-11-20T16:15:24.445180Z",
     "shell.execute_reply.started": "2024-11-20T16:02:53.617072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [12:30<00:00,  1.33it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.24676048878644827,\n",
       " 'rouge2': 0.07045343781907064,\n",
       " 'rougeL': 0.18139636217125518,\n",
       " 'rougeLsum': 0.1814796955045885}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=answersumm['test']\n",
    "final_sums=[]\n",
    "final_summ_scores=[]\n",
    "true_sums=[]\n",
    "for sample in tqdm(test_data):\n",
    "    ref_summ=sample['summaries'][0][1]\n",
    "    query=sample['question']['question']\n",
    "    sents=[]\n",
    "    for ans in sample['answers']:\n",
    "        for sent in ans['sents']:\n",
    "            sents.append(sent['text'])\n",
    "    random.shuffle(sents)\n",
    "    final_summ=generate_summary(' '.join([x for x in sents]))\n",
    "    final_sums.append(final_summ)\n",
    "    true_sums.append(ref_summ)\n",
    "    final_summ_scores.append(compute_rouge_score(final_summ,ref_summ))\n",
    "    \n",
    "sum_dict = defaultdict(int)\n",
    "count_dict = defaultdict(int)\n",
    "for d in final_summ_scores:\n",
    "    for key, value in d.items():\n",
    "        sum_dict[key] += value\n",
    "        count_dict[key] += 1\n",
    "average_dict = {key: sum_val / count_dict[key] for key, sum_val in sum_dict.items()}\n",
    "average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:29:42.857307Z",
     "iopub.status.busy": "2024-11-18T20:29:42.856579Z",
     "iopub.status.idle": "2024-11-18T20:51:13.153685Z",
     "shell.execute_reply": "2024-11-18T20:51:13.152857Z",
     "shell.execute_reply.started": "2024-11-18T20:29:42.857271Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [21:30<00:00,  1.29s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.21996755451800554,\n",
       " 'rouge2': 0.05378383290463431,\n",
       " 'rougeL': 0.16190530965770017,\n",
       " 'rougeLsum': 0.16196501115023748}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=answersumm['test']\n",
    "final_sums=[]\n",
    "final_summ_scores=[]\n",
    "for sample in tqdm(test_data):\n",
    "    test_dict={}\n",
    "    ref_summ=sample['summaries'][0][1]\n",
    "    query=sample['question']['question']\n",
    "    q_emb=model.encode(query,show_progress_bar=False)\n",
    "    sent_lst=[]\n",
    "    for ans in sample['answers']:\n",
    "        for sent in ans['sents']:\n",
    "            sent_lst.append(sent['text'])\n",
    "    sent_emb=model.encode(sent_lst,show_progress_bar=False)\n",
    "    for sen,sen_emb in zip(sent_lst,sent_emb):\n",
    "        test_dict[tuple(sen_emb)]={'sentence':sen, 'query': query, 'q_emb': q_emb}\n",
    "    # DBSCAN\n",
    "    data=list(test_dict.keys())\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
    "    labels = dbscan.fit_predict(data)\n",
    "    grouped = defaultdict(list)\n",
    "    for emb, clust_id in zip(data, labels):\n",
    "        if clust_id!=-1:  \n",
    "            grouped[clust_id].append(test_dict[emb]['sentence'])\n",
    "    # within group ranking and summ\n",
    "    result = list(grouped.values())\n",
    "    group_summaries=[]\n",
    "    for group in result:\n",
    "        sent_scores={}\n",
    "        for sent in group:\n",
    "            inputs = tokenizer_relreg(query, sent, \n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\")\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            sent_score = model_relreg(input_ids, attention_mask, token_type_ids).item()\n",
    "            sent_scores[sent]=sent_score\n",
    "        sorted_dict = sorted(sent_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        top_5_sents = ' '.join([key for key, _ in sorted_dict[:3]])\n",
    "        group_summaries.append(top_5_sents)\n",
    "    final_summ=generate_summary(' '.join([x for x in group_summaries]))\n",
    "    final_sums.append(final_summ)\n",
    "    final_summ_scores.append(compute_rouge_score(final_summ,ref_summ))\n",
    "\n",
    "sum_dict = defaultdict(int)\n",
    "count_dict = defaultdict(int)\n",
    "for d in final_summ_scores:\n",
    "    for key, value in d.items():\n",
    "        sum_dict[key] += value\n",
    "        count_dict[key] += 1\n",
    "average_dict = {key: sum_val / count_dict[key] for key, sum_val in sum_dict.items()}\n",
    "average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T20:51:26.391156Z",
     "iopub.status.busy": "2024-11-18T20:51:26.390801Z",
     "iopub.status.idle": "2024-11-18T21:13:59.653854Z",
     "shell.execute_reply": "2024-11-18T21:13:59.652941Z",
     "shell.execute_reply.started": "2024-11-18T20:51:26.391127Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [22:33<00:00,  1.35s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.23835263676056226,\n",
       " 'rouge2': 0.0653611800576848,\n",
       " 'rougeL': 0.17550960634175589,\n",
       " 'rougeLsum': 0.1756511992621099}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=answersumm['test']\n",
    "final_sums=[]\n",
    "final_summ_scores=[]\n",
    "for sample in tqdm(test_data):\n",
    "    test_dict={}\n",
    "    ref_summ=sample['summaries'][0][1]\n",
    "    query=sample['question']['question']\n",
    "    q_emb=model.encode(query,show_progress_bar=False)\n",
    "    sent_lst=[]\n",
    "    for ans in sample['answers']:\n",
    "        for sent in ans['sents']:\n",
    "            sent_lst.append(sent['text'])\n",
    "    sent_emb=model.encode(sent_lst,show_progress_bar=False)\n",
    "    for sen,sen_emb in zip(sent_lst,sent_emb):\n",
    "        test_dict[tuple(sen_emb)]={'sentence':sen, 'query': query, 'q_emb': q_emb}\n",
    "    # DBSCAN\n",
    "    data=list(test_dict.keys())\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
    "    labels = dbscan.fit_predict(data)\n",
    "    grouped = defaultdict(list)\n",
    "    for emb, clust_id in zip(data, labels):\n",
    "        if clust_id!=-1:  \n",
    "            grouped[clust_id].append(test_dict[emb]['sentence'])\n",
    "    # within group ranking and summ\n",
    "    result = list(grouped.values())\n",
    "    group_summaries=[]\n",
    "    for group in result:\n",
    "        sent_scores={}\n",
    "        for sent in group:\n",
    "            inputs = tokenizer_relreg(query, sent, \n",
    "                                max_length=512, \n",
    "                                padding=\"max_length\", \n",
    "                                truncation=True, \n",
    "                                return_tensors=\"pt\")\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            token_type_ids = inputs['token_type_ids'].to(device)\n",
    "            sent_score = model_relreg(input_ids, attention_mask, token_type_ids).item()\n",
    "            sent_scores[sent]=sent_score\n",
    "        sorted_dict = sorted(sent_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        top_5_sents = ' '.join([key for key, _ in sorted_dict[:7]])\n",
    "        group_summaries.append(top_5_sents)\n",
    "    final_summ=generate_summary(' '.join([x for x in group_summaries]))\n",
    "    final_sums.append(final_summ)\n",
    "    final_summ_scores.append(compute_rouge_score(final_summ,ref_summ))\n",
    "\n",
    "sum_dict = defaultdict(int)\n",
    "count_dict = defaultdict(int)\n",
    "for d in final_summ_scores:\n",
    "    for key, value in d.items():\n",
    "        sum_dict[key] += value\n",
    "        count_dict[key] += 1\n",
    "average_dict = {key: sum_val / count_dict[key] for key, sum_val in sum_dict.items()}\n",
    "average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T21:14:52.104576Z",
     "iopub.status.busy": "2024-11-18T21:14:52.104227Z",
     "iopub.status.idle": "2024-11-18T21:33:27.114163Z",
     "shell.execute_reply": "2024-11-18T21:33:27.113242Z",
     "shell.execute_reply.started": "2024-11-18T21:14:52.104547Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [18:34<00:00,  1.11s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.24211679479813913,\n",
       " 'rouge2': 0.06914819376243228,\n",
       " 'rougeL': 0.17978250493986755,\n",
       " 'rougeLsum': 0.17984311100047362}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=answersumm['test']\n",
    "final_sums=[]\n",
    "final_summ_scores=[]\n",
    "for sample in tqdm(test_data):\n",
    "    test_dict={}\n",
    "    ref_summ=sample['summaries'][0][1]\n",
    "    query=sample['question']['question']\n",
    "    q_emb=model.encode(query,show_progress_bar=False)\n",
    "    sent_lst=[]\n",
    "    for ans in sample['answers']:\n",
    "        for sent in ans['sents']:\n",
    "            sent_lst.append(sent['text'])\n",
    "    sent_emb=model.encode(sent_lst,show_progress_bar=False)\n",
    "    for sen,sen_emb in zip(sent_lst,sent_emb):\n",
    "        test_dict[tuple(sen_emb)]={'sentence':sen, 'query': query, 'q_emb': q_emb}\n",
    "    # DBSCAN\n",
    "    data=list(test_dict.keys())\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
    "    labels = dbscan.fit_predict(data)\n",
    "    grouped = defaultdict(list)\n",
    "    for emb, clust_id in zip(data, labels):\n",
    "        if clust_id!=-1:  \n",
    "            grouped[clust_id].append(test_dict[emb]['sentence'])\n",
    "    # within group ranking and summ\n",
    "    result = list(grouped.values())\n",
    "    group_summaries=[]\n",
    "    for group in result:\n",
    "        sent_scores={}\n",
    "        for sent in group:\n",
    "            sentence_embedding = model.encode(sent,show_progress_bar=False)\n",
    "            sent_score = (cosine_similarity(q_emb.reshape(1, -1),sentence_embedding.reshape(1,-1)).tolist())[0]\n",
    "            sent_scores[sent]=sent_score\n",
    "        sorted_dict = sorted(sent_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        top_5_sents = ' '.join([key for key, _ in sorted_dict[:5]])\n",
    "        group_summaries.append(top_5_sents)\n",
    "    final_summ=generate_summary(' '.join([x for x in group_summaries]))\n",
    "    final_sums.append(final_summ)\n",
    "    final_summ_scores.append(compute_rouge_score(final_summ,ref_summ))\n",
    "\n",
    "sum_dict = defaultdict(int)\n",
    "count_dict = defaultdict(int)\n",
    "for d in final_summ_scores:\n",
    "    for key, value in d.items():\n",
    "        sum_dict[key] += value\n",
    "        count_dict[key] += 1\n",
    "average_dict = {key: sum_val / count_dict[key] for key, sum_val in sum_dict.items()}\n",
    "average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_data=answersumm['test']\n",
    "final_sums=[]\n",
    "final_summ_scores=[]\n",
    "for sample in tqdm(test_data):\n",
    "    test_dict={}\n",
    "    ref_summ=sample['summaries'][0][1]\n",
    "    query=sample['question']['question']\n",
    "    q_emb=model.encode(query,show_progress_bar=False)\n",
    "    sent_lst=[]\n",
    "    for ans in sample['answers']:\n",
    "        for sent in ans['sents']:\n",
    "            sent_lst.append(sent['text'])\n",
    "    sent_emb=model.encode(sent_lst,show_progress_bar=False)\n",
    "    for sen,sen_emb in zip(sent_lst,sent_emb):\n",
    "        test_dict[tuple(sen_emb)]={'sentence':sen, 'query': query, 'q_emb': q_emb}\n",
    "    # DBSCAN\n",
    "    data=list(test_dict.keys())\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
    "    labels = dbscan.fit_predict(data)\n",
    "    grouped = defaultdict(list)\n",
    "    for emb, clust_id in zip(data, labels):\n",
    "        if clust_id!=-1:  \n",
    "            grouped[clust_id].append(test_dict[emb]['sentence'])\n",
    "    # within group ranking and summ\n",
    "    result = list(grouped.values())\n",
    "    group_summaries=[]\n",
    "    for group in result:\n",
    "        sent_scores={}\n",
    "        for sent in group:\n",
    "            sentence_embedding = model.encode(sent,show_progress_bar=False)\n",
    "            sent_score = (cosine_similarity(q_emb.reshape(1, -1),sentence_embedding.reshape(1,-1)).tolist())[0]\n",
    "            sent_scores[sent]=sent_score\n",
    "        sorted_dict = sorted(sent_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        top_5_sents = ' '.join([key for key, _ in sorted_dict[:3]])\n",
    "        group_summaries.append(top_5_sents)\n",
    "    final_summ=generate_summary(' '.join([x for x in group_summaries]))\n",
    "    final_sums.append(final_summ)\n",
    "    final_summ_scores.append(compute_rouge_score(final_summ,ref_summ))\n",
    "\n",
    "sum_dict = defaultdict(int)\n",
    "count_dict = defaultdict(int)\n",
    "for d in final_summ_scores:\n",
    "    for key, value in d.items():\n",
    "        sum_dict[key] += value\n",
    "        count_dict[key] += 1\n",
    "average_dict = {key: sum_val / count_dict[key] for key, sum_val in sum_dict.items()}\n",
    "average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T16:48:12.685416Z",
     "iopub.status.busy": "2024-11-19T16:48:12.685059Z",
     "iopub.status.idle": "2024-11-19T17:07:58.488447Z",
     "shell.execute_reply": "2024-11-19T17:07:58.487580Z",
     "shell.execute_reply.started": "2024-11-19T16:48:12.685385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [19:45<00:00,  1.19s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.25354954841498867,\n",
       " 'rouge2': 0.07353783573612624,\n",
       " 'rougeL': 0.1861692653576872,\n",
       " 'rougeLsum': 0.18621959774135294}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=answersumm['test']\n",
    "final_sums=[]\n",
    "final_summ_scores=[]\n",
    "for sample in tqdm(test_data):\n",
    "    test_dict={}\n",
    "    ref_summ=sample['summaries'][0][1]\n",
    "    query=sample['question']['question']\n",
    "    q_emb=model.encode(query,show_progress_bar=False)\n",
    "    sent_lst=[]\n",
    "    for ans in sample['answers']:\n",
    "        for sent in ans['sents']:\n",
    "            sent_lst.append(sent['text'])\n",
    "    sent_emb=model.encode(sent_lst,show_progress_bar=False)\n",
    "    for sen,sen_emb in zip(sent_lst,sent_emb):\n",
    "        test_dict[tuple(sen_emb)]={'sentence':sen, 'query': query, 'q_emb': q_emb}\n",
    "    # DBSCAN\n",
    "    data=list(test_dict.keys())\n",
    "    dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
    "    labels = dbscan.fit_predict(data)\n",
    "    grouped = defaultdict(list)\n",
    "    for emb, clust_id in zip(data, labels):\n",
    "        if clust_id!=-1:  \n",
    "            grouped[clust_id].append(test_dict[emb]['sentence'])\n",
    "    # within group ranking and summ\n",
    "    result = list(grouped.values())\n",
    "    group_summaries=[]\n",
    "    for group in result:\n",
    "        sent_scores={}\n",
    "        for sent in group:\n",
    "            sentence_embedding = model.encode(sent,show_progress_bar=False)\n",
    "            sent_score = (cosine_similarity(q_emb.reshape(1, -1),sentence_embedding.reshape(1,-1)).tolist())[0]\n",
    "            sent_scores[sent]=sent_score\n",
    "        sorted_dict = sorted(sent_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "        top_5_sents = ' '.join([key for key, _ in sorted_dict])\n",
    "        group_summaries.append(top_5_sents)\n",
    "    final_summ=generate_summary(' '.join([x for x in group_summaries]))\n",
    "    final_sums.append(final_summ)\n",
    "    final_summ_scores.append(compute_rouge_score(final_summ,ref_summ))\n",
    "\n",
    "sum_dict = defaultdict(int)\n",
    "count_dict = defaultdict(int)\n",
    "for d in final_summ_scores:\n",
    "    for key, value in d.items():\n",
    "        sum_dict[key] += value\n",
    "        count_dict[key] += 1\n",
    "average_dict = {key: sum_val / count_dict[key] for key, sum_val in sum_dict.items()}\n",
    "average_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T17:41:21.766080Z",
     "iopub.status.busy": "2024-11-17T17:41:21.765671Z",
     "iopub.status.idle": "2024-11-17T17:41:21.772165Z",
     "shell.execute_reply": "2024-11-17T17:41:21.771210Z",
     "shell.execute_reply.started": "2024-11-17T17:41:21.766034Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are a number of ways to succeed taunt kills that is connected to choosing the right targets, distraction and prediction of their moves. Snipers seem to be the easiest target.'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T17:41:21.773827Z",
     "iopub.status.busy": "2024-11-17T17:41:21.773516Z",
     "iopub.status.idle": "2024-11-17T17:41:21.783524Z",
     "shell.execute_reply": "2024-11-17T17:41:21.782631Z",
     "shell.execute_reply.started": "2024-11-17T17:41:21.773795Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"They will focus down their scope for days, completely oblivious to the world around them, feeling perfectly safe wearing their razorbacks.Even huntsmen snipers tend to focus just before a shot.Pick your target well.If you're being chased, and you think you can time it right, round a corner, wait if necessary, then perform the taunt.If your goal is specifically to get a taunt kill, you may want to favor certain classes.You are not likely to be able to\""
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T17:56:31.307919Z",
     "iopub.status.busy": "2024-11-17T17:56:31.307534Z",
     "iopub.status.idle": "2024-11-17T17:56:32.874416Z",
     "shell.execute_reply": "2024-11-17T17:56:32.873405Z",
     "shell.execute_reply.started": "2024-11-17T17:56:31.307886Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Playing as the Heavy with the Holiday Punch gloves equipped, you can hit enemies from behind to make the enemy laugh. While they're laughing you can easily taunt-kill them If you are playing as a Heavy, you should be able to taunt kill most enemies. There are two relatively easy ways to do this, that works with most taunt kills. Snipers. Sneak up behind them, preferably a sniper as they tend not to move too much, and give em a good whack.\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.2608695652173913,\n",
       " 'rouge2': 0.05309734513274336,\n",
       " 'rougeL': 0.13913043478260867,\n",
       " 'rougeLsum': 0.13913043478260867}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for sample in test_data.select(range(99,100)):\n",
    "    ref_summ=sample['summaries'][0][1]\n",
    "    query=sample['question']['question']\n",
    "    inp_str=[]\n",
    "    for ans in sample['answers']:\n",
    "        for sent in ans['sents']:\n",
    "            inp_str.append(sent['text'])\n",
    "    random.shuffle(inp_str)\n",
    "    inp_str=' '.join(inp_str)\n",
    "    inputs = tokenizer2(inp_str,\n",
    "            max_length=1024,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            )\n",
    "    with torch.no_grad():\n",
    "        output_ids = model2.generate(\n",
    "                input_ids=inputs[\"input_ids\"].to(device),\n",
    "                attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                max_length=100,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "        )\n",
    "    s=[tokenizer2.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
    "    print(s)\n",
    "compute_rouge_score(s[0],ref_summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T17:35:25.787355Z",
     "iopub.status.busy": "2024-11-17T17:35:25.786604Z",
     "iopub.status.idle": "2024-11-17T17:35:25.798139Z",
     "shell.execute_reply": "2024-11-17T17:35:25.797109Z",
     "shell.execute_reply.started": "2024-11-17T17:35:25.787312Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Demomen are best against a sentry farm. Heavies or pyros can also be effective. The medic should lead the uber combo. If possible, back the uber team(s) up with as much raw firepower as you can muster. Go with corner-edging soldiers and sneaky demos.  The Machina can help damage Engies that hide behind their sentry, but for unattended sentries, the Sydney Sleeper charges up very fast.  The best way you can help your team is by building up a teleporter, so that they can get to and take down the other team's Sentry.    \n",
      "\n",
      "In reponse to a question about the Turret Song on Reddit , the author of the song \"The Potatos Lament\" stated as follows:In reply to the same question, the author answered:I can hear 'lacrimosa' at the beginning of the Lament.Translate it, you get 'Crying or Tearful Potato\".Potato Lacrimosa (Weeping Potato) Potato Po Uota (Power Potato Vows) Diva MePotato Lament (Lament) Potato Lamentation (Crying Potato) 'Potato' to be exact.As for the lyrics, she's probably singing in Italian...Still trying to get the lyrics myself if it is indeed in Italian.If it is in Italian, I'm not sure how to interpret it.I think it sounds like she's repeating the lyrics to Vitrification Order on the Second Volume.I'll let you determine if that's the song she's singing.\"GLa {'rouge1': 0.06756756756756756, 'rouge2': 0.0136986301369863, 'rougeL': 0.04054054054054054, 'rougeLsum': 0.04054054054054054}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for x in final_sums:\n",
    "#     print(x)\n",
    "#     print()\n",
    "c=0\n",
    "for i,x in enumerate(zip(final_sums,final_summ_scores)):\n",
    "    if x[1]['rougeL']<0.15:\n",
    "        c+=1\n",
    "        print(i)\n",
    "        print(answersumm['test'][i]['summaries'][0][1])\n",
    "        print()\n",
    "        print(x[0],x[1])\n",
    "        print()\n",
    "        break\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:12:40.202981Z",
     "iopub.status.busy": "2024-11-17T16:12:40.202501Z",
     "iopub.status.idle": "2024-11-17T16:12:40.217620Z",
     "shell.execute_reply": "2024-11-17T16:12:40.216566Z",
     "shell.execute_reply.started": "2024-11-17T16:12:40.202934Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Labels: [ 0  1  1  1 -1 -1  1 -1  1  0  1  1 -1 -1 -1  1 -1  1  1 -1  1 -1  1 -1\n",
      " -1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "data=list(test_dict.keys())\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=2, metric='cosine')\n",
    "labels = dbscan.fit_predict(data)\n",
    "print(\"Cluster Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:37:34.852555Z",
     "iopub.status.busy": "2024-11-17T16:37:34.851991Z",
     "iopub.status.idle": "2024-11-17T16:37:34.866032Z",
     "shell.execute_reply": "2024-11-17T16:37:34.864646Z",
     "shell.execute_reply.started": "2024-11-17T16:37:34.852493Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I've tried fixing an Xbox 360 myself by doing the X-Clamp replacement but that fix only lasted about 8 months before it red ringed again.\",\n",
       " 'Since Microsoft paid 1.5 billion dollars for you to have a 3 year warranty for your Xbox 360, you should just have them fix it for you for free.',\n",
       " \"That is, assuming you've had this Xbox 360 less than 3 years.\",\n",
       " \"Your 360 might run for about 10 minutes, and you've just voided any warranty you may have had.\",\n",
       " 'I would not advise opening your Xbox 360 to fix the RRoD. Unless a lot has changed in a few years, the fixes I found for RRoD only made the problem worse.',\n",
       " 'I was personally never able to get my RRoD console fixed with solutions found on the internet.',\n",
       " 'And then I found that once I had opened it, Microsoft would not even touch the console for any amount of money.',\n",
       " 'Worked for me when I got the RRoD',\n",
       " \"As an owner of a video games repair shop, the kits do work only for certain fixes, but they definitely won't last as long as a reflow of the board.\",\n",
       " 'Consider the kits as temporary fixes for some red ring of death related issues.',\n",
       " 'If you just apply the kit fix and it overheats again, it just means it would require a reflow of the chip instead.',\n",
       " \"it's the heat that's causing the issue, so its always best to mod the 360 internal fan to keep it cooler.\",\n",
       " 'I know many people do their own research and find forums regarding this RROD repair topic, but I must honestly say there is a lot of false and misleading information from others that have fixed their own systems or only a handful of them.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = defaultdict(list)\n",
    "for emb, clust_id in zip(data, labels):\n",
    "    grouped[clust_id].append(test_dict[emb]['sentence'])\n",
    "grouped[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:12:40.231985Z",
     "iopub.status.busy": "2024-11-17T16:12:40.231455Z",
     "iopub.status.idle": "2024-11-17T16:12:40.239886Z",
     "shell.execute_reply": "2024-11-17T16:12:40.238847Z",
     "shell.execute_reply.started": "2024-11-17T16:12:40.231943Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' It voids your warranty if you open up the console. Microsoft give a 3 year warranty with Xbox 360, you should just have them fix it for you for free. I would not advise opening your Xbox 360 to fix the RRoD. Unless a lot has changed in a few years, the fixes for RRoD only make the problem worse.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersumm['test'][1]['summaries'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:12:40.241269Z",
     "iopub.status.busy": "2024-11-17T16:12:40.240853Z",
     "iopub.status.idle": "2024-11-17T16:12:40.251057Z",
     "shell.execute_reply": "2024-11-17T16:12:40.250113Z",
     "shell.execute_reply.started": "2024-11-17T16:12:40.241220Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've heard of kits that can fix your Xbox 360 if you happen to get the  Red Ring of Death . Do any of these work, and do they void your warranty? Are there any other noteworthy tips or tricks?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answersumm['test'][1]['question']['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:13:17.572770Z",
     "iopub.status.busy": "2024-11-17T16:13:17.572385Z",
     "iopub.status.idle": "2024-11-17T16:13:17.579643Z",
     "shell.execute_reply": "2024-11-17T16:13:17.578624Z",
     "shell.execute_reply.started": "2024-11-17T16:13:17.572733Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RelRegModel(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super(RelRegModel, self).__init__()\n",
    "        self.encoder = BertModel.from_pretrained(model_name)\n",
    "        self.regressor = nn.Linear(self.encoder.config.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.encoder(input_ids=input_ids, \n",
    "                               attention_mask=attention_mask, \n",
    "                               token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        score = self.regressor(pooled_output)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_relreg=torch.load('../input/anlp-relreg/RelReg.pth')\n",
    "tokenizer_relreg = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:38:07.998993Z",
     "iopub.status.busy": "2024-11-17T16:38:07.998620Z",
     "iopub.status.idle": "2024-11-17T16:38:08.620210Z",
     "shell.execute_reply": "2024-11-17T16:38:08.619286Z",
     "shell.execute_reply.started": "2024-11-17T16:38:07.998959Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "curr_data=answersumm['test']\n",
    "sample_dict={}\n",
    "for sample in tqdm(curr_data.select(range(1,2))):\n",
    "    ref_summ=sample['summaries'][0][1]\n",
    "    query=sample['question']['question']\n",
    "    sents=[]\n",
    "    temp_scores=[]\n",
    "    for ans in sample['answers']:\n",
    "        for sent in ans['sents']:\n",
    "            inputs = tokenizer_relreg(query, sent['text'], \n",
    "                            max_length=512, \n",
    "                            padding=\"max_length\", \n",
    "                            truncation=True, \n",
    "                            return_tensors=\"pt\")\n",
    "            sents.append(sent['text'])\n",
    "            curr={\n",
    "                'input_ids': inputs['input_ids'],\n",
    "                'attention_mask': inputs['attention_mask'],\n",
    "                'token_type_ids': inputs['token_type_ids'],\n",
    "            }\n",
    "\n",
    "            input_ids = curr['input_ids'].to(device)\n",
    "            attention_mask = curr['attention_mask'].to(device)\n",
    "            token_type_ids = curr['token_type_ids'].to(device)\n",
    "            outputs = model_relreg(input_ids, attention_mask, token_type_ids)\n",
    "            sample_dict[sent['text']]=outputs.item()\n",
    "sample_dict=sorted(sample_dict.items(), key=lambda item: item[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:38:15.337595Z",
     "iopub.status.busy": "2024-11-17T16:38:15.336872Z",
     "iopub.status.idle": "2024-11-17T16:38:15.344619Z",
     "shell.execute_reply": "2024-11-17T16:38:15.343733Z",
     "shell.execute_reply.started": "2024-11-17T16:38:15.337553Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Since Microsoft paid 1.5 billion dollars for you to have a 3 year warranty for your Xbox 360, you should just have them fix it for you for free.',\n",
       "  0.19985000789165497),\n",
       " ('Consider the kits as temporary fixes for some red ring of death related issues.',\n",
       "  0.18345361948013306),\n",
       " (\"Your 360 might run for about 10 minutes, and you've just voided any warranty you may have had.\",\n",
       "  0.18208950757980347),\n",
       " ('It voids your warranty if you open up the console.', 0.16600573062896729),\n",
       " ('You may hear of something called the towel trick if you have googled this.',\n",
       "  0.1521153450012207),\n",
       " ('I would not advise opening your Xbox 360 to fix the RRoD. Unless a lot has changed in a few years, the fixes I found for RRoD only made the problem worse.',\n",
       "  0.1493116319179535),\n",
       " (\"As an owner of a video games repair shop, the kits do work only for certain fixes, but they definitely won't last as long as a reflow of the board.\",\n",
       "  0.14652936160564423),\n",
       " ('Let alone it voids your warranty.', 0.14080893993377686),\n",
       " (\"I don't have any experience with the kits you're looking at, but try unplugging your AC adaptor, dusting it out, moving it to a spot with more ventilation, and plugging it back in.\",\n",
       "  0.12811297178268433),\n",
       " ('Whatever you do, do not try this.', 0.11697849631309509),\n",
       " (\", it's mostly about the customer support and UPS experience, but you get the picture: STAY AWAY http://www.greyhats.com/gaming/why-my-xbox-360-support-has-sucked-40\",\n",
       "  0.11063322424888611),\n",
       " ('Any board that has the dreaded red ring of death just means the board is prone to heat and gives it a higher chance of it overheating again.',\n",
       "  0.10637520998716354),\n",
       " (\"I've tried fixing an Xbox 360 myself by doing the X-Clamp replacement but that fix only lasted about 8 months before it red ringed again.\",\n",
       "  0.1040845662355423),\n",
       " ('I know many people do their own research and find forums regarding this RROD repair topic, but I must honestly say there is a lot of false and misleading information from others that have fixed their own systems or only a handful of them.',\n",
       "  0.10296693444252014),\n",
       " ('If you just apply the kit fix and it overheats again, it just means it would require a reflow of the chip instead.',\n",
       "  0.09919752180576324),\n",
       " ('You will need to know how to solder and have a soldering iron.',\n",
       "  0.0966762900352478),\n",
       " (\"Other than that, I don't know of any solutions that work.\",\n",
       "  0.09144334495067596),\n",
       " (\"it's the heat that's causing the issue, so its always best to mod the 360 internal fan to keep it cooler.\",\n",
       "  0.08595991134643555),\n",
       " (\"That is, assuming you've had this Xbox 360 less than 3 years.\",\n",
       "  0.08461495488882065),\n",
       " (\"Most RROD occurrence's is different, which is why you will need to find the error code to find the culprit chip.\",\n",
       "  0.08393928408622742),\n",
       " ('And then I found that once I had opened it, Microsoft would not even touch the console for any amount of money.',\n",
       "  0.07979020476341248),\n",
       " (\"Here's a very good article on the RROD subject on the differences of a reflow and reball .\",\n",
       "  0.0695604681968689),\n",
       " ('I was personally never able to get my RRoD console fixed with solutions found on the internet.',\n",
       "  0.06266739219427109),\n",
       " (\"- apparently it's just a generic error indicator for a number of problems.\",\n",
       "  0.056918516755104065),\n",
       " ('Keep in mind', 0.05574846267700195),\n",
       " ('This my blog about the experience', 0.038259975612163544),\n",
       " ('Worked for me when I got the RRoD', 0.011612839996814728)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:23:51.675287Z",
     "iopub.status.busy": "2024-11-17T16:23:51.674633Z",
     "iopub.status.idle": "2024-11-17T16:24:00.364594Z",
     "shell.execute_reply": "2024-11-17T16:24:00.363769Z",
     "shell.execute_reply.started": "2024-11-17T16:23:51.675246Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e232c297e49945a7b147f0447f462c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e41e393c39247988178164ce70ef0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4551bd8559654dcd9786f5be807de024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21fd33edea74a8bb0d2a3e75e65c352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f6cc53a7194400811dcd7ab32d5082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a1124a0b744c3199bb1692230c0c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.02G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"facebook/bart-large\"\n",
    "tokenizer2 = BartTokenizer.from_pretrained(model_name)\n",
    "model2 = BartForConditionalGeneration.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T17:40:20.259822Z",
     "iopub.status.busy": "2024-11-17T17:40:20.258930Z",
     "iopub.status.idle": "2024-11-17T17:40:20.265391Z",
     "shell.execute_reply": "2024-11-17T17:40:20.264405Z",
     "shell.execute_reply.started": "2024-11-17T17:40:20.259780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_summary(input_text):\n",
    "    inputs = tokenizer2(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    summary_ids = model2.generate(inputs[\"input_ids\"].to(device), max_length=100, min_length=10, length_penalty=2.0, num_beams=4)\n",
    "    summary = tokenizer2.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:37:58.947961Z",
     "iopub.status.busy": "2024-11-17T16:37:58.947197Z",
     "iopub.status.idle": "2024-11-17T16:38:06.967233Z",
     "shell.execute_reply": "2024-11-17T16:38:06.966280Z",
     "shell.execute_reply.started": "2024-11-17T16:37:58.947921Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consider the kits as temporary fixes for some red ring of death related issues.You may hear of something called the towel trick if you have googled this.It voids your warranty if you open up the console.Your 360 might run for about 10 minutes, and you've just voided any warranty you may have had.Since Microsoft paid 1.5 billion dollars for you to have a 3 year warranty for your Xbox 360, you should just have them fix it for you for free.\n",
      "I don't have any experience with the kits you're looking at, but try unplugging your AC adaptor, dusting it out, moving it to a spot with more ventilation, and plugging it back in.Your 360 might run for about 10 minutes, and you've just voided any warranty you may have had.As an owner of a video games repair shop, the kits do work only for certain fixes, but they definitely won't last as long as a reflow of the board.Consider the kits as temporary fixes for some red ring of death related issues. Unless a lot has changed in a few years, the fixes I found for RRoD only made the problem worse.Since Microsoft paid 1.5 billion dollars for you to have a 3 year warranty for your Xbox 360, you should just have them fix it for you for free.It voids your warranty if you open up the console.You may hear of something called the towel trick if you\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:07<00:00,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know many people do their own research and find forums regarding this RROD repair topic, but I must honestly say there is a lot of false and misleading information from others that have fixed their own systems or only a handful of them.I've tried fixing an Xbox 360 myself by doing the X-Clamp replacement but that fix only lasted about 8 months before it red ringed again. Unless a lot has changed in a few years, the fixes I found for RRoD only made the problem worse.I don't have any experience with the kits you're looking at, but try unplugging your AC adaptor, dusting it out, moving it to a spot with more ventilation, and plugging it back in.Your 360 might run for about 10 minutes, and you've just voided any warranty you may have had.Consider the kits as temporary fixes for some red ring of death related issues.As an owner of a video games repair shop, the kits do\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "curr_data=answersumm['test']\n",
    "scores_5=[]\n",
    "scores_10=[]\n",
    "scores_15=[]\n",
    "for sample in tqdm(curr_data.select(range(1,2))):\n",
    "    ref_summ=sample['summaries'][0][1]\n",
    "    query=sample['question']['question']\n",
    "    sents=[]\n",
    "    temp_scores=[]\n",
    "    for ans in sample['answers']:\n",
    "        for sent in ans['sents']:\n",
    "            inputs = tokenizer_relreg(query, sent['text'], \n",
    "                            max_length=512, \n",
    "                            padding=\"max_length\", \n",
    "                            truncation=True, \n",
    "                            return_tensors=\"pt\")\n",
    "            sents.append(sent['text'])\n",
    "            curr={\n",
    "                'input_ids': inputs['input_ids'],\n",
    "                'attention_mask': inputs['attention_mask'],\n",
    "                'token_type_ids': inputs['token_type_ids'],\n",
    "            }\n",
    "\n",
    "            input_ids = curr['input_ids'].to(device)\n",
    "            attention_mask = curr['attention_mask'].to(device)\n",
    "            token_type_ids = curr['token_type_ids'].to(device)\n",
    "            outputs = model_relreg(input_ids, attention_mask, token_type_ids)\n",
    "            temp_scores.extend(outputs.detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "    sorted_strings = [string for _, string in sorted(zip(temp_scores, sents), reverse=True)]\n",
    "    inp_str=\"\"\n",
    "    for s in sorted_strings[:5]:\n",
    "        inp_str+=s\n",
    "    temp=generate_summary(inp_str)\n",
    "    scores_5.append(compute_rouge_score(temp,ref_summ))\n",
    "    print(temp)\n",
    "    inp_str=\"\"\n",
    "    for s in sorted_strings[:10]:\n",
    "        inp_str+=s\n",
    "    temp=generate_summary(inp_str)\n",
    "    scores_10.append(compute_rouge_score(temp,ref_summ))\n",
    "    print(temp)\n",
    "    inp_str=\"\"\n",
    "    for s in sorted_strings[:15]:\n",
    "        inp_str+=s\n",
    "    temp=generate_summary(inp_str)\n",
    "    scores_15.append(compute_rouge_score(temp,ref_summ))\n",
    "    print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-17T16:29:32.878263Z",
     "iopub.status.busy": "2024-11-17T16:29:32.877508Z",
     "iopub.status.idle": "2024-11-17T16:29:32.885208Z",
     "shell.execute_reply": "2024-11-17T16:29:32.884321Z",
     "shell.execute_reply.started": "2024-11-17T16:29:32.878210Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'rouge1': 0.4761904761904762,\n",
       "   'rouge2': 0.3586206896551724,\n",
       "   'rougeL': 0.380952380952381,\n",
       "   'rougeLsum': 0.380952380952381}],\n",
       " [{'rouge1': 0.44347826086956527,\n",
       "   'rouge2': 0.3508771929824561,\n",
       "   'rougeL': 0.22608695652173916,\n",
       "   'rougeLsum': 0.22608695652173916}],\n",
       " [{'rouge1': 0.35497835497835495,\n",
       "   'rouge2': 0.13973799126637554,\n",
       "   'rougeL': 0.20779220779220778,\n",
       "   'rougeLsum': 0.20779220779220778}])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_5,scores_10,scores_15"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6106612,
     "sourceId": 9933695,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6115334,
     "sourceId": 9945204,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
