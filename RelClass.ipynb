{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9925451,"sourceType":"datasetVersion","datasetId":6100525},{"sourceId":9944691,"sourceType":"datasetVersion","datasetId":6114944}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate rouge-score > /dev/null 2>&1;","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-18T19:30:40.789150Z","iopub.execute_input":"2024-11-18T19:30:40.789502Z","iopub.status.idle":"2024-11-18T19:30:52.401541Z","shell.execute_reply.started":"2024-11-18T19:30:40.789469Z","shell.execute_reply":"2024-11-18T19:30:52.400376Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nimport torch\nimport torch.nn as nn\nfrom transformers import BertTokenizer, BertModel\nimport evaluate\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import r2_score\nfrom tqdm import tqdm\nfrom transformers import BartForConditionalGeneration, BartTokenizer\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-11-18T19:30:52.403283Z","iopub.execute_input":"2024-11-18T19:30:52.403543Z","iopub.status.idle":"2024-11-18T19:31:09.835078Z","shell.execute_reply.started":"2024-11-18T19:30:52.403519Z","shell.execute_reply":"2024-11-18T19:31:09.834163Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"rouge=evaluate.load(\"rouge\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-11-18T19:31:09.836165Z","iopub.execute_input":"2024-11-18T19:31:09.836688Z","iopub.status.idle":"2024-11-18T19:31:11.228432Z","shell.execute_reply.started":"2024-11-18T19:31:09.836642Z","shell.execute_reply":"2024-11-18T19:31:11.227560Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"118565216c9d425781b4fa78079fd190"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"answersumm = load_dataset(\"alexfabbri/answersumm\")","metadata":{"execution":{"iopub.status.busy":"2024-11-18T19:31:11.230720Z","iopub.execute_input":"2024-11-18T19:31:11.231087Z","iopub.status.idle":"2024-11-18T19:31:13.902881Z","shell.execute_reply.started":"2024-11-18T19:31:11.231049Z","shell.execute_reply":"2024-11-18T19:31:13.902205Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/9.74k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9814a4b7681443809e24de944f396a32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train.jsonl:   0%|          | 0.00/24.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"044d2d7ea7474d80a6ac40a5f8af0ab1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation.jsonl:   0%|          | 0.00/4.43M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"198916f872fe40d0abea7187241dd0dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test.jsonl:   0%|          | 0.00/8.76M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"301d98b421a142a4883350724978077e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2783 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a073ef1b461a4cf6921a39e44a0736e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ed9a182bc240d497d276137e907adc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"963b3792f4564636a1595acc0c35464f"}},"metadata":{}}]},{"cell_type":"code","source":"train_data=answersumm['train']\nquery_dataset=[]\nsentence_dataset=[]\nlabel_dataset=[]\nsummary_dataset=[]\nfor sample in train_data:\n    ref_summ=sample['summaries'][0][1]\n    query=sample['question']['question']\n    for ans in sample['answers']:\n        for sent in ans['sents']:\n            clust_id=sent['cluster_id'][0][0]\n            label_dataset.append(0 if clust_id==-1 else 1)\n            sentence_dataset.append(sent['text'])\n            query_dataset.append(query)\n            summary_dataset.append(ref_summ)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T16:13:31.309696Z","iopub.execute_input":"2024-11-16T16:13:31.309998Z","iopub.status.idle":"2024-11-16T16:13:35.024407Z","shell.execute_reply.started":"2024-11-16T16:13:31.309966Z","shell.execute_reply":"2024-11-16T16:13:35.023233Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def compute_rouge_score(sent,ref_summary):\n    results=rouge.compute(predictions=[sent], references=[ref_summary])\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-11-18T19:31:13.903767Z","iopub.execute_input":"2024-11-18T19:31:13.904032Z","iopub.status.idle":"2024-11-18T19:31:13.908196Z","shell.execute_reply.started":"2024-11-18T19:31:13.904006Z","shell.execute_reply":"2024-11-18T19:31:13.907371Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class RelRegDataset(Dataset):\n    def __init__(self, tokenizer, max_len, queries, sentences, labels, summaries):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.queries=queries\n        self.labels=labels\n        self.sentences=sentences\n        self.summaries=summaries\n\n    def __len__(self):\n        return len(self.queries)\n\n    def __getitem__(self, idx):\n        sentence = self.sentences[idx]\n        query = self.queries[idx]\n        target = self.labels[idx]\n        \n        inputs = self.tokenizer(query, sentence, \n                                max_length=self.max_len, \n                                padding=\"max_length\", \n                                truncation=True, \n                                return_tensors=\"pt\")\n        return {\n            'input_ids': inputs['input_ids'].squeeze(0),\n            'attention_mask': inputs['attention_mask'].squeeze(0),\n            'token_type_ids': inputs['token_type_ids'].squeeze(0),\n            'targets': torch.tensor(target, dtype=torch.float),\n            'summ': self.summaries[idx],\n        }","metadata":{"execution":{"iopub.status.busy":"2024-11-18T19:31:13.909345Z","iopub.execute_input":"2024-11-18T19:31:13.909706Z","iopub.status.idle":"2024-11-18T19:31:13.922123Z","shell.execute_reply.started":"2024-11-18T19:31:13.909648Z","shell.execute_reply":"2024-11-18T19:31:13.921303Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class RelRegModel(nn.Module):\n    def __init__(self, model_name=\"bert-base-uncased\"):\n        super(RelRegModel, self).__init__()\n        self.encoder = BertModel.from_pretrained(model_name)\n        self.regressor = nn.Linear(self.encoder.config.hidden_size, 1)\n    \n    def forward(self, input_ids, attention_mask, token_type_ids):\n        outputs = self.encoder(input_ids=input_ids, \n                               attention_mask=attention_mask, \n                               token_type_ids=token_type_ids)\n        pooled_output = outputs.pooler_output\n        score = torch.sigmoid(self.regressor(pooled_output))\n        return score","metadata":{"execution":{"iopub.status.busy":"2024-11-18T19:31:13.923245Z","iopub.execute_input":"2024-11-18T19:31:13.923589Z","iopub.status.idle":"2024-11-18T19:31:13.933330Z","shell.execute_reply.started":"2024-11-18T19:31:13.923554Z","shell.execute_reply":"2024-11-18T19:31:13.932495Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"model=RelRegModel()\nmodel=model.to(device)\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\ncriterion = nn.BCELoss()\nepochs=5","metadata":{"execution":{"iopub.status.busy":"2024-11-18T19:31:13.934322Z","iopub.execute_input":"2024-11-18T19:31:13.934635Z","iopub.status.idle":"2024-11-18T19:31:15.137219Z","shell.execute_reply.started":"2024-11-18T19:31:13.934601Z","shell.execute_reply":"2024-11-18T19:31:15.136365Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"122445fe23dc4e4fa70f3970eebdf089"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543e4278197d4fbd8b6046b35d5168f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"680367c98c4b423895785fb90392e99c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c2c1f4c57144268902f3e213b80e60a"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset=RelRegDataset(tokenizer,512,query_dataset[:30000],sentence_dataset[:30000],label_dataset[:30000],summary_dataset[:30000])\ntrain_dataloader = DataLoader(train_dataset, batch_size=40, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-11-16T16:13:54.640088Z","iopub.execute_input":"2024-11-16T16:13:54.640427Z","iopub.status.idle":"2024-11-16T16:13:54.651253Z","shell.execute_reply.started":"2024-11-16T16:13:54.640387Z","shell.execute_reply":"2024-11-16T16:13:54.650328Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model.train()\nfor epoch in range(epochs):\n    total_loss=0\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        targets = batch['targets'].to(device)\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, token_type_ids)\n        loss = criterion(outputs.squeeze(-1), targets)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    train_loss=total_loss/len(train_dataloader)\n    print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T16:13:54.652413Z","iopub.execute_input":"2024-11-16T16:13:54.652718Z","iopub.status.idle":"2024-11-16T18:31:24.236959Z","shell.execute_reply.started":"2024-11-16T16:13:54.652685Z","shell.execute_reply":"2024-11-16T18:31:24.236099Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Epoch 1/5:  30%|███       | 227/750 [08:19<19:12,  2.20s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nEpoch 1/5: 100%|██████████| 750/750 [27:29<00:00,  2.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.3924\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/5:  66%|██████▋   | 497/750 [18:13<09:15,  2.19s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nEpoch 2/5: 100%|██████████| 750/750 [27:28<00:00,  2.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Loss: 0.2779\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5:  71%|███████▏  | 535/750 [19:37<07:52,  2.20s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nEpoch 3/5: 100%|██████████| 750/750 [27:30<00:00,  2.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3, Loss: 0.1538\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/5:  54%|█████▍    | 407/750 [14:55<12:35,  2.20s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nEpoch 4/5: 100%|██████████| 750/750 [27:30<00:00,  2.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4, Loss: 0.0749\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5:  18%|█▊        | 137/750 [05:01<22:32,  2.21s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nEpoch 5/5: 100%|██████████| 750/750 [27:30<00:00,  2.20s/it]","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Loss: 0.0474\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, \"RelRegRel_30k.pth\")","metadata":{"execution":{"iopub.status.busy":"2024-11-16T18:34:54.483598Z","iopub.execute_input":"2024-11-16T18:34:54.483910Z","iopub.status.idle":"2024-11-16T18:34:55.124355Z","shell.execute_reply.started":"2024-11-16T18:34:54.483877Z","shell.execute_reply":"2024-11-16T18:34:55.123551Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model=torch.load('../input/anlp-relreg10k/RelRegRel_10k.pth')","metadata":{"execution":{"iopub.status.busy":"2024-11-18T19:31:15.138259Z","iopub.execute_input":"2024-11-18T19:31:15.138506Z","iopub.status.idle":"2024-11-18T19:31:20.094099Z","shell.execute_reply.started":"2024-11-18T19:31:15.138483Z","shell.execute_reply":"2024-11-18T19:31:20.093383Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2489527215.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model=torch.load('../input/anlp-relreg10k/RelRegRel_10k.pth')\n","output_type":"stream"}]},{"cell_type":"code","source":"model_name = \"facebook/bart-large\"\ntokenizer2 = BartTokenizer.from_pretrained(model_name)\n# model2 = BartForConditionalGeneration.from_pretrained(model_name).to(device)\nmodel2=torch.load('../input/bart-ft2/BART_FT2.pth')\nmodel2=model2.to(device)\n\ndef generate_summary(input_text):\n    inputs = tokenizer2(input_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n    summary_ids = model2.generate(inputs[\"input_ids\"].to(device), max_length=256, min_length=10, length_penalty=2.0, num_beams=4)\n    summary = tokenizer2.decode(summary_ids[0], skip_special_tokens=True)\n    return summary","metadata":{"execution":{"iopub.status.busy":"2024-11-18T19:31:20.096298Z","iopub.execute_input":"2024-11-18T19:31:20.096571Z","iopub.status.idle":"2024-11-18T19:31:29.624025Z","shell.execute_reply.started":"2024-11-18T19:31:20.096545Z","shell.execute_reply":"2024-11-18T19:31:29.623313Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87c51e3ee1eb496b9fa31df8ffe6e7c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b0d76ebffa649d9993f5ff23c55e666"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75e3f4c336ab4fe39bd51302f77da0cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cda63e6873d74e50a7879e81cccfc04f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7be459d4e1c8402bba9519127fba35d1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/tmp/ipykernel_30/2863228842.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model2=torch.load('../input/bart-ft2/BART_FT2.pth')\n","output_type":"stream"}]},{"cell_type":"code","source":"model.eval()\ncurr_data=answersumm['test']\nscores_5=[]\nscores_10=[]\nscores_15=[]\nfor sample in tqdm(curr_data):\n    ref_summ=sample['summaries'][0][1]\n    query=sample['question']['question']\n    sents=[]\n    temp_scores=[]\n    for ans in sample['answers']:\n        for sent in ans['sents']:\n            inputs = tokenizer(query, sent['text'], \n                            max_length=512, \n                            padding=\"max_length\", \n                            truncation=True, \n                            return_tensors=\"pt\")\n            curr={\n                'input_ids': inputs['input_ids'],\n                'attention_mask': inputs['attention_mask'],\n                'token_type_ids': inputs['token_type_ids'],\n            }\n\n            input_ids = curr['input_ids'].to(device)\n            attention_mask = curr['attention_mask'].to(device)\n            token_type_ids = curr['token_type_ids'].to(device)\n            outputs = model(input_ids, attention_mask, token_type_ids)\n            if outputs.item()>=0.5:\n                temp_scores.append(outputs.item())\n                sents.append(sent['text'])\n\n    sorted_strings = [string for _, string in sorted(zip(temp_scores, sents), reverse=True)]\n    inp_str=\"\"\n    for s in sorted_strings[:5]:\n        inp_str+=s\n    scores_5.append(compute_rouge_score(generate_summary(inp_str),ref_summ))\n    inp_str=\"\"\n    for s in sorted_strings[:10]:\n        inp_str+=s\n    scores_10.append(compute_rouge_score(generate_summary(inp_str),ref_summ))\n    inp_str=\"\"\n    for s in sorted_strings[:15]:\n        inp_str+=s\n    scores_15.append(compute_rouge_score(generate_summary(inp_str),ref_summ))","metadata":{"execution":{"iopub.status.busy":"2024-11-18T19:31:29.625065Z","iopub.execute_input":"2024-11-18T19:31:29.625342Z","iopub.status.idle":"2024-11-18T20:12:20.215552Z","shell.execute_reply.started":"2024-11-18T19:31:29.625317Z","shell.execute_reply":"2024-11-18T20:12:20.214721Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"100%|██████████| 1000/1000 [40:50<00:00,  2.45s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import defaultdict\n\nsum_dict = defaultdict(int)\ncount_dict = defaultdict(int)\n\n# Iterate through the list of dictionaries\nfor d in scores_5:\n    for key, value in d.items():\n        sum_dict[key] += value\n        count_dict[key] += 1\n\n# Calculate the average for each key\naverage_dict = {key: sum_val / count_dict[key] for key, sum_val in sum_dict.items()}\naverage_dict","metadata":{"execution":{"iopub.status.busy":"2024-11-18T20:12:20.216881Z","iopub.execute_input":"2024-11-18T20:12:20.217437Z","iopub.status.idle":"2024-11-18T20:12:20.226239Z","shell.execute_reply.started":"2024-11-18T20:12:20.217394Z","shell.execute_reply":"2024-11-18T20:12:20.225339Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 0.2147745246976527,\n 'rouge2': 0.05856866825743714,\n 'rougeL': 0.16374635433129672,\n 'rougeLsum': 0.16387968766463004}"},"metadata":{}}]},{"cell_type":"code","source":"sum_dict = defaultdict(int)\ncount_dict = defaultdict(int)\n\n# Iterate through the list of dictionaries\nfor d in scores_10:\n    for key, value in d.items():\n        sum_dict[key] += value\n        count_dict[key] += 1\n\n# Calculate the average for each key\naverage_dict = {key: sum_val / count_dict[key] for key, sum_val in sum_dict.items()}\naverage_dict","metadata":{"execution":{"iopub.status.busy":"2024-11-18T20:12:20.227258Z","iopub.execute_input":"2024-11-18T20:12:20.227513Z","iopub.status.idle":"2024-11-18T20:12:20.244453Z","shell.execute_reply.started":"2024-11-18T20:12:20.227489Z","shell.execute_reply":"2024-11-18T20:12:20.243735Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 0.22294463049895938,\n 'rouge2': 0.061920629106610316,\n 'rougeL': 0.1680199544942722,\n 'rougeLsum': 0.16817390638430657}"},"metadata":{}}]},{"cell_type":"code","source":"sum_dict = defaultdict(int)\ncount_dict = defaultdict(int)\n\n# Iterate through the list of dictionaries\nfor d in scores_15:\n    for key, value in d.items():\n        sum_dict[key] += value\n        count_dict[key] += 1\n\n# Calculate the average for each key\naverage_dict = {key: sum_val / count_dict[key] for key, sum_val in sum_dict.items()}\naverage_dict","metadata":{"execution":{"iopub.status.busy":"2024-11-18T20:12:20.245240Z","iopub.execute_input":"2024-11-18T20:12:20.245454Z","iopub.status.idle":"2024-11-18T20:12:20.258399Z","shell.execute_reply.started":"2024-11-18T20:12:20.245428Z","shell.execute_reply":"2024-11-18T20:12:20.257706Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'rouge1': 0.2239469318831701,\n 'rouge2': 0.06326262523183113,\n 'rougeL': 0.17020881118329514,\n 'rougeLsum': 0.1704042134821457}"},"metadata":{}}]}]}